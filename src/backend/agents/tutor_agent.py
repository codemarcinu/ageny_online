"""
Tutor Antonina Agent.
Agent edukacyjny pomagajÄ…cy w tworzeniu skutecznych promptÃ³w.
"""

import logging
from typing import Dict, List, Optional, Any
from backend.core.llm_providers.provider_factory import provider_factory as llm_factory
from backend.agents.base_agent import BaseAgent, AgentConfig, AgentContext, AgentResponse

logger = logging.getLogger(__name__)

SYSTEM_PROMPT = """
JesteÅ› AntoninÄ… â€“ doÅ›wiadczonÄ… trenerkÄ… promptÃ³w. Pomagaj uczennicy tworzyÄ‡ peÅ‚ne, skuteczne prompty, uwzglÄ™dniajÄ…c te 6 elementÃ³w:

1. **Kontekst** - tÅ‚o sytuacji, cel i okolicznoÅ›ci
2. **Instrukcja** - konkretne zadanie do wykonania
3. **Ograniczenia** - co NIE robiÄ‡, ograniczenia techniczne
4. **Format odpowiedzi** - jak ma wyglÄ…daÄ‡ odpowiedÅº
5. **PrzykÅ‚ady** - wzorce lub przykÅ‚ady oczekiwanych rezultatÃ³w
6. **System prompt** - rola i styl komunikacji AI

**Zasady dziaÅ‚ania:**
- JeÅ›li brakuje ktÃ³regoÅ› z 6 elementÃ³w, zadaj JEDNO pytanie doprecyzowujÄ…ce ten konkretny element
- Pytanie powinno byÄ‡ konkretne i prowadziÄ‡ do uzupeÅ‚nienia brakujÄ…cego elementu
- Gdy wszystkie 6 elementÃ³w sÄ… obecne, rozpocznij odpowiedÅº od "Sugestia:" i podaj zwiÄ™zÅ‚Ä… sugestiÄ™ ulepszenia
- NastÄ™pnie podaj "Ulepszony prompt:" i zoptymalizowanÄ… wersjÄ™ prompta
- BÄ…dÅº przyjazna, ale profesjonalna - jak doÅ›wiadczona nauczycielka

**PrzykÅ‚ad pytania:** "W jakim kontekÅ›cie chcesz uÅ¼yÄ‡ tego prompta? Czy to zadanie szkolne, praca, czy coÅ› innego?"

**PrzykÅ‚ad odpowiedzi z sugestiÄ…:**
Sugestia: TwÃ³j prompt jest dobry, ale dodaj konkretny format odpowiedzi.

Ulepszony prompt: [zoptymalizowana wersja]
"""


class TutorAntonina(BaseAgent):
    """Agent edukacyjny pomagajÄ…cy w tworzeniu skutecznych promptÃ³w."""
    
    def __init__(self, model: Optional[str] = None, provider: Optional[str] = None):
        config = AgentConfig(
            agent_type="tutor_antonina",
            name="Tutor Antonina",
            description="Edukacyjny agent pomagajÄ…cy w tworzeniu skutecznych promptÃ³w",
            priority=2,
            max_concurrent_requests=5,
            timeout_seconds=45
        )
        super().__init__(config)
        self.model = model
        self.provider = provider
    
    async def guide(self, last_prompt: str, chat_history: List[Dict[str, str]]) -> Dict[str, Optional[str]]:
        """
        Przewodnik po tworzeniu promptÃ³w.
        
        Args:
            last_prompt: Ostatni prompt uÅ¼ytkownika
            chat_history: Historia rozmowy
            
        Returns:
            Dict z pytaniem lub feedbackiem
        """
        try:
            # Przygotuj wiadomoÅ›ci dla LLM
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"Przeanalizuj ten prompt i pomÃ³Å¼ mi go ulepszyÄ‡:\n\n{last_prompt}"}
            ]
            
            # Dodaj kontekst z historii jeÅ›li istnieje
            if chat_history:
                context = "\n\nKontekst z poprzednich wiadomoÅ›ci:\n"
                for msg in chat_history[-3:]:  # Ostatnie 3 wiadomoÅ›ci
                    context += f"- {msg['role']}: {msg['content'][:100]}...\n"
                messages[1]["content"] += context
            
            # Wybierz provider
            if self.provider:
                provider_type = None
                for pt in llm_factory.get_available_providers():
                    if pt.value == self.provider:
                        provider_type = pt
                        break
                
                if not provider_type:
                    logger.warning(f"Provider {self.provider} not available, using fallback")
                    result = await llm_factory.chat_with_fallback(
                        messages=messages,
                        model=self.model,
                        temperature=0.2,
                        max_tokens=400
                    )
                else:
                    provider = llm_factory.get_provider(provider_type)
                    result = await provider.chat(
                        messages=messages,
                        model=self.model,
                        temperature=0.2,
                        max_tokens=400
                    )
            else:
                result = await llm_factory.chat_with_fallback(
                    messages=messages,
                    model=self.model,
                    temperature=0.2,
                    max_tokens=400
                )
            
            # Handle different response formats
            if isinstance(result, str):
                content = result.strip()
            elif isinstance(result, dict):
                if "text" in result:
                    content = result["text"].strip()
                elif "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"].strip()
                else:
                    content = str(result).strip()
            else:
                content = str(result).strip()
            
            # Analizuj odpowiedÅº
            if content.startswith("Sugestia:"):
                # Prompt jest kompletny - zwrÃ³Ä‡ feedback
                parts = content.split("Ulepszony prompt:")
                suggestion = parts[0].replace("Sugestia:", "").strip()
                improved = parts[1].strip() if len(parts) > 1 else ""
                
                feedback = f"{suggestion}\n\nUlepszony prompt:\n{improved}"
                return {"question": None, "feedback": feedback}
            else:
                # Brakuje elementÃ³w - zwrÃ³Ä‡ pytanie
                return {"question": content, "feedback": None}
                
        except Exception as e:
            logger.error(f"Tutor guide error: {e}")
            return {
                "question": "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d. SprÃ³buj ponownie sformuÅ‚owaÄ‡ swÃ³j prompt.",
                "feedback": None
            }
    
    async def process_query(
        self, 
        query: str, 
        context: Optional[AgentContext] = None,
        **kwargs: Any
    ) -> AgentResponse:
        """
        Przetwarzanie zapytania przez tutora.
        
        Args:
            query: Prompt do analizy
            context: Kontekst agenta
            **kwargs: Dodatkowe parametry
            
        Returns:
            OdpowiedÅº agenta
        """
        start_time = self._get_current_time()
        
        try:
            # Pobierz historiÄ™ z kontekstu jeÅ›li dostÄ™pna
            chat_history = context.metadata.get("chat_history", []) if context else []
            
            # Przewodnik po promptach
            guide_result = await self.guide(query, chat_history)
            
            # Przygotuj odpowiedÅº
            if guide_result["question"]:
                content = f"ðŸ¤” **Pytanie od Tutora Antoniny:**\n\n{guide_result['question']}"
            else:
                content = f"âœ… **Sugestia od Tutora Antoniny:**\n\n{guide_result['feedback']}"
            
            processing_time = self._get_current_time() - start_time
            
            return AgentResponse(
                success=True,
                content=content,
                agent_type=self.config.agent_type,
                provider_used=self.provider,
                processing_time=processing_time,
                metadata={
                    "tutor_question": guide_result["question"],
                    "tutor_feedback": guide_result["feedback"]
                }
            )
            
        except Exception as e:
            logger.error(f"Tutor process_query error: {e}")
            self.error_count += 1
            self.last_error = str(e)
            
            return AgentResponse(
                success=False,
                content=f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas analizy prompta: {str(e)}",
                agent_type=self.config.agent_type,
                processing_time=self._get_current_time() - start_time
            )
    
    def _get_current_time(self) -> float:
        """Pobierz aktualny czas."""
        import time
        return time.time() 